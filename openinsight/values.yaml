openinsight:
  enabled: true
  # Default values for openinsight-helm-chart.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  clickhouse:
    enabled: true
    service:
      type: NodePort
    zookeeper:
      enabled: true
      replicaCount: 1
    shards: 1
    replicaCount: 1
    resources:
      requests:
        cpu: 2
        memory: 4Gi
      limits:
        cpu: 4
        memory: 8Gi
    auth:
      username: default
      password: "changeme"
    extraEnvVars:
      - name: TZ
        value: "UTC"
    tls:
      enabled: false
    initdbScripts:
      my_init_script.sh: |
        #!/bin/bash
        set -e
        
        clickhouse client -u default --password='changeme' -n <<-EOSQL
          CREATE DATABASE openinsight;
        EOSQL
    defaultConfigurationOverrides: |
      <clickhouse>
        <!-- Macros -->
        <macros>
          <shard from_env="CLICKHOUSE_SHARD_ID"></shard>
          <replica from_env="CLICKHOUSE_REPLICA_ID"></replica>
          <layer>{{ include "common.names.fullname" . }}</layer>
        </macros>
        <!-- Log Level -->
        <logger>
          <level>{{ .Values.logLevel }}</level>
        </logger>
        {{- if or (ne (int .Values.shards) 1) (ne (int .Values.replicaCount) 1)}}
        <!-- Cluster configuration - Any update of the shards and replicas requires helm upgrade -->
        <remote_servers>
          <secret>foo</secret>
          <default>
            {{- $shards := $.Values.shards | int }}
            {{- range $shard, $e := until $shards }}
            <shard>
                {{- $replicas := $.Values.replicaCount | int }}
                {{- range $i, $_e := until $replicas }}
                <replica>
                    <host>{{ printf "%s-shard%d-%d.%s.%s.svc.%s" (include "common.names.fullname" $ ) $shard $i (include "clickhouse.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
                    <port>{{ $.Values.service.ports.tcp }}</port>
                </replica>
                {{- end }}
            </shard>
            {{- end }}
          </default>
        </remote_servers>
        {{- end }}
        {{- if or .Values.zookeeper.enabled .Values.externalZookeeper.servers }}
        <!-- Zookeeper configuration -->
        <zookeeper>
          {{- if .Values.zookeeper.enabled }}
          {{/* Zookeeper configuration using the helm chart */}}
          {{- $nodes := .Values.zookeeper.replicaCount | int }}
          {{- range $node, $e := until $nodes }}
          <node>
            <host>{{ printf "%s-%d.%s.%s.svc.%s" (include "clickhouse.zookeeper.fullname" $ ) $node (include "clickhouse.zookeeper.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
            <port>{{ $.Values.zookeeper.service.ports.client }}</port>
          </node>
          {{- end }}
          {{- else if .Values.externalZookeeper.servers }}
          {{/* Zookeeper configuration using an external instance */}}
          {{- range $node :=.Values.externalZookeeper.servers }}
          <node>
            <host>{{ $node }}</host>
            <port>{{ $.Values.externalZookeeper.port }}</port>
          </node>
          {{- end }}
          {{- end }}
        </zookeeper>
        {{- end }}
        {{- if .Values.tls.enabled }}
        <!-- TLS configuration -->
        <tcp_port_secure from_env="CLICKHOUSE_TCP_SECURE_PORT"></tcp_port_secure>
        <openSSL>
            <server>
                {{- $certFileName := default "tls.crt" .Values.tls.certFilename }}
                {{- $keyFileName := default "tls.key" .Values.tls.certKeyFilename }}
                <certificateFile>/bitnami/clickhouse/certs/{{$certFileName}}</certificateFile>
                <privateKeyFile>/bitnami/clickhouse/certs/{{$keyFileName}}</privateKeyFile>
                <verificationMode>none</verificationMode>
                <cacheSessions>true</cacheSessions>
                <disableProtocols>sslv2,sslv3</disableProtocols>
                <preferServerCiphers>true</preferServerCiphers>
                {{- if or .Values.tls.autoGenerated .Values.tls.certCAFilename }}
                {{- $caFileName := default "ca.crt" .Values.tls.certFilename }}
                <caConfig>/bitnami/clickhouse/certs/{{$caFileName}}</caConfig>
                {{- else }}
                <loadDefaultCAFile>true</loadDefaultCAFile>
                {{- end }}
            </server>
            <client>
                <loadDefaultCAFile>true</loadDefaultCAFile>
                <cacheSessions>true</cacheSessions>
                <disableProtocols>sslv2,sslv3</disableProtocols>
                <preferServerCiphers>true</preferServerCiphers>
                <verificationMode>none</verificationMode>
                <invalidCertificateHandler>
                    <name>AcceptCertificateHandler</name>
                </invalidCertificateHandler>
            </client>
        </openSSL>
        {{- end }}
        {{- if .Values.metrics.enabled }}
         <!-- Prometheus metrics -->
         <prometheus>
            <endpoint>/metrics</endpoint>
            <port from_env="CLICKHOUSE_METRICS_PORT"></port>
            <metrics>true</metrics>
            <events>true</events>
            <asynchronous_metrics>true</asynchronous_metrics>
        </prometheus>
        {{- end }}
      </clickhouse>
  opentelemetry-collector:
    enabled: true
    nameOverride: "openinsight"
    mode: "deployment"
    image:
      # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
      repository: ghcr.m.daocloud.io/openinsight-proj/openinsight
      pullPolicy: IfNotPresent
      tag: "1ab7d4556f1e63e818a8cf4a37dd967887653f99"
    presets:
      # Configures the collector to collect logs.
      # Adds the filelog receiver to the logs pipeline
      # and adds the necessary volumes and volume mounts.
      # Best used with mode = daemonset.
      logsCollection:
        enabled: false
        includeCollectorLogs: false
      # Configures the collector to collect host metrics.
      # Adds the hostmetrics receiver to the metrics pipeline
      # and adds the necessary volumes and volume mounts.
      # Best used with mode = daemonset.
      hostMetrics:
        enabled: false
      # Configures the Kubernetes Processor to add Kubernetes metadata.
      # Adds the k8sattributes processor to all the pipelines
      # and adds the necessary rules to ClusteRole.
      # Best used with mode = daemonset.
      kubernetesAttributes:
        enabled: false
      # Configures the Kubernetes Cluster Receiver to collect cluster-level metrics.
      # Adds the k8s_cluster receiver to the metrics pipeline
      # and adds the necessary rules to ClusteRole.
      # Best used with mode = deployment or statefulset.
      clusterMetrics:
        enabled: false
      # Configures the collector to collect Kubelet metrics.
      # Adds the kubeletstats receiver to the metrics pipeline
      # and adds the necessary rules to ClusteRole.
      # Best used with mode = daemonset.
      kubeletMetrics:
        enabled: false

    configMap:
      # Specifies whether a configMap should be created (true by default)
      create: true
    config:
      exporters:
        logging: {}
        prometheus:
          endpoint: "0.0.0.0:8889"
        clickhouse:
          dsn: tcp://default:changeme@{{ .Release.Name }}-clickhouse-headless:9000/openinsight
          logs_table_name: openinsight_logs
          traces_table_name: openinsight_traces
          metrics_table_name: openinsight_metrics
          ttl_days: 3
          timeout: 10s
          sending_queue:
            queue_size: 100
          retry_on_failure:
            enabled: true
            initial_interval: 5s
            max_interval: 30s
            max_elapsed_time: 300s
      extensions:
        # The health_check extension is mandatory for this chart.
        # Without the health_check extension the collector will fail the readiness and liveliness probes.
        # The health_check extension can be modified, but should never be removed.
        health_check: {}
        memory_ballast: {}
        query:
          protocols:
            http:
              endpoint: 0.0.0.0:18888
            grpc:
              endpoint: 0.0.0.0:18889
          storage:
            clickhouse:
              dsn: tcp://default:changeme@{{ .Release.Name }}-clickhouse-headless:9000/openinsight
              logging_table_name: openinsight_logs
              tracing_table_name: openinsight_traces
              metrics_table_name: openinsight_metrics
              tls:
                insecure: true
                insecure_skip_verify: true
          tracing_query:
            storage_type: clickhouse
          logging_query:
            storage_type: clickhouse
          metrics_query:
            storage_type: clickhouse
      processors:
        batch: {}
        batch/clickhouse:
          send_batch_size: 30000
          timeout: 10s
        # If set to null, will be overridden with values based on k8s resource limits
        memory_limiter: null
        spanmetrics:
          metrics_exporter: prometheus
          latency_histogram_buckets: [2ms, 4ms, 6ms, 8ms, 10ms, 50ms, 100ms, 200ms, 500ms, 800ms, 1s, 1400ms, 2s, 5s, 10s, 15s]
          aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"
          dimensions_cache_size: 4000
        servicegraph:
          metrics_exporter: prometheus
          latency_histogram_buckets: [2ms, 4ms, 6ms, 8ms, 10ms, 50ms, 100ms, 200ms, 500ms, 800ms, 1s, 1400ms, 2s, 5s, 10s, 15s]
          store:
            ttl: 10s
            max_items: 100000
      receivers:
        jaeger:
          protocols:
            grpc:
              endpoint: 0.0.0.0:14250
            thrift_http:
              endpoint: 0.0.0.0:14268
            thrift_compact:
              endpoint: 0.0.0.0:6831
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318
        prometheus:
          config:
            scrape_configs:
              - job_name: openinsight-collector
                scrape_interval: 5s
                static_configs:
                  - targets:
                      - ${MY_POD_IP}:8888
              - job_name: span-metrics
                scrape_interval: 5s
                static_configs:
                  - targets:
                      - ${MY_POD_IP}:8889
        # Dummy receiver that's never used, because a pipeline is required to have one.
        otlp/otel_metrics:
          protocols:
            grpc:
              endpoint: "localhost:65535"
        zipkin:
          endpoint: 0.0.0.0:9411
        fluentforward:
          endpoint: 0.0.0.0:8006
      service:
        telemetry:
          logs:
            level: info
          metrics:
            address: 0.0.0.0:8888
        extensions:
          - memory_ballast
          - query
          - health_check
        pipelines:
          logs/ck:
            exporters:
              - logging
              - clickhouse
            processors:
              - memory_limiter
              - batch/clickhouse
            receivers:
              - otlp
              - fluentforward
          metrics/span_metrics:
            exporters:
              - prometheus
            receivers:
              - otlp/otel_metrics
          metrics:
            exporters:
              - logging
              - clickhouse
            processors:
              - memory_limiter
              - batch/clickhouse
            receivers:
              - otlp
              - prometheus
          traces:
            exporters:
              - logging
              - clickhouse
            processors:
              - memory_limiter
              - servicegraph
              - spanmetrics
              - batch/clickhouse
            receivers:
              - otlp
              - jaeger
              - zipkin

    ports:
      query-http:
        enabled: true
        containerPort: 18888
        servicePort: 18888
        hostPort: 18888
        protocol: TCP
      query-grpc:
        enabled: true
        containerPort: 18889
        servicePort: 18889
        hostPort: 18889
        protocol: TCP
      otlp:
        enabled: true
        containerPort: 4317
        servicePort: 4317
        hostPort: 4317
        protocol: TCP
      otlp-http:
        enabled: true
        containerPort: 4318
        servicePort: 4318
        hostPort: 4318
        protocol: TCP
      jaeger-compact:
        enabled: true
        containerPort: 6831
        servicePort: 6831
        hostPort: 6831
        protocol: UDP
      jaeger-thrift:
        enabled: true
        containerPort: 14268
        servicePort: 14268
        hostPort: 14268
        protocol: TCP
      jaeger-grpc:
        enabled: true
        containerPort: 14250
        servicePort: 14250
        hostPort: 14250
        protocol: TCP
      zipkin:
        enabled: true
        containerPort: 9411
        servicePort: 9411
        hostPort: 9411
        protocol: TCP
      metrics:
        # The metrics port is disabled by default. However you need to enable the port
        # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
        enabled: true
        containerPort: 8888
        servicePort: 8888
        protocol: TCP
      span-metrics:
        # The metrics port is disabled by default. However you need to enable the port
        # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
        enabled: true
        containerPort: 8889
        servicePort: 8889
        protocol: TCP
      fluent-forward:
        enabled: true
        containerPort: 8006
        servicePort: 8006
        hostPort: 8006
        protocol: TCP

    # Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
    resources:
      limits:
        cpu: 1000m
        memory: 1024Mi

    service:
      type: NodePort
